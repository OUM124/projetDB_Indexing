{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5G-pQacLh-W",
        "outputId": "5f0f558a-7981-4ade-8d6d-1cd02e875577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbEvkhtJLpiG",
        "outputId": "84b37c7f-4ba1-4b79-8c47-89f327f1f516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys in the HDF5 file: ['distances', 'neighbors', 'test', 'train']\n",
            "Training data shape: (1000000, 960)\n",
            "Query data shape: (1000, 960)\n",
            "Ground truth shape: (1000, 100)\n",
            "Distance truth shape: (1000, 100)\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the HDF5 file\n",
        "with h5py.File('/content/drive/MyDrive/gist-960-euclidean.hdf5', 'r') as f:\n",
        "    # List all keys (datasets) in the file\n",
        "    print(\"Keys in the HDF5 file:\", list(f.keys()))\n",
        "\n",
        "    # Read the training set, queries, ground truth, and distances\n",
        "    train_data = f['train'][:]  # Assuming 'train' is the key for training data\n",
        "    query_data = f['test'][:]  # Assuming 'test' is the key for query data\n",
        "    ground_truth = f['neighbors'][:]  # Assuming 'neighbors' is the key for ground truth neighbors\n",
        "    distance = f['distances'][:]  # Assuming 'distances' is the key for ground truth distances\n",
        "\n",
        "# Inspect the data\n",
        "print(\"Training data shape:\", train_data.shape)\n",
        "print(\"Query data shape:\", query_data.shape)\n",
        "print(\"Ground truth shape:\", ground_truth.shape)\n",
        "print(\"Distance truth shape:\", distance.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mBFCww15K5M7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple, Optional, Any\n",
        "from collections import defaultdict\n",
        "import heapq\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import threading\n",
        "\n",
        "class MTreeNode:\n",
        "    def __init__(self, data=None, radius=0, parent=None, distance_to_parent=0):\n",
        "        self.data = data\n",
        "        self.radius = radius\n",
        "        self.parent = parent\n",
        "        self.distance_to_parent = distance_to_parent\n",
        "        self.routing_entries = []\n",
        "        self.is_leaf = True\n",
        "        self._distance_cache = {}\n",
        "        self._lock = threading.Lock()\n",
        "\n",
        "    def add_routing_entry(self, child, distance):\n",
        "        self.routing_entries.append((child, distance))\n",
        "        self.is_leaf = False\n",
        "\n",
        "class MTree:\n",
        "    def __init__(self, max_node_size=4, distance_metric='euclidean'):\n",
        "        self.root = None\n",
        "        self.max_node_size = max_node_size\n",
        "        self.distance_metric = distance_metric\n",
        "        self._distance_cache = {}\n",
        "        self._cache_lock = threading.Lock()\n",
        "\n",
        "    def _compute_distance(self, x1: np.ndarray, x2: np.ndarray) -> float:\n",
        "        key = tuple(sorted([hash(x1.tobytes()), hash(x2.tobytes())]))\n",
        "\n",
        "        with self._cache_lock:\n",
        "            if key in self._distance_cache:\n",
        "                return self._distance_cache[key]\n",
        "\n",
        "            if self.distance_metric == 'euclidean':\n",
        "                dist = np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "            else:\n",
        "                dist = np.linalg.norm(x1 - x2)\n",
        "\n",
        "            self._distance_cache[key] = dist\n",
        "            return dist\n",
        "\n",
        "    def bulk_load(self, data: np.ndarray, batch_size: int = 10000):\n",
        "        print(\"Starting bulk load...\")\n",
        "        n_samples = len(data)\n",
        "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
        "\n",
        "        def process_batch(batch_data):\n",
        "            return MTreeNode(data=batch_data)\n",
        "\n",
        "        nodes = []\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            futures = []\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                batch = data[i:i + batch_size]\n",
        "                futures.append(executor.submit(process_batch, batch))\n",
        "\n",
        "            for future in futures:\n",
        "                nodes.append(future.result())\n",
        "\n",
        "        while len(nodes) > 1:\n",
        "            new_nodes = []\n",
        "            for i in range(0, len(nodes), self.max_node_size):\n",
        "                batch = nodes[i:i + self.max_node_size]\n",
        "                if len(batch) == 1:\n",
        "                    new_nodes.append(batch[0])\n",
        "                    continue\n",
        "\n",
        "                parent = MTreeNode()\n",
        "                pivot = batch[0]\n",
        "                parent.data = pivot.data\n",
        "\n",
        "                def compute_distances(node):\n",
        "                    return self._compute_distance(parent.data, node.data)\n",
        "\n",
        "                with ThreadPoolExecutor() as executor:\n",
        "                    distances = list(executor.map(compute_distances, batch))\n",
        "\n",
        "                for node, distance in zip(batch, distances):\n",
        "                    node.parent = parent\n",
        "                    node.distance_to_parent = distance\n",
        "                    parent.add_routing_entry(node, distance)\n",
        "\n",
        "                parent.radius = max(distances)\n",
        "                new_nodes.append(parent)\n",
        "\n",
        "            nodes = new_nodes\n",
        "            print(f\"Created {len(nodes)} nodes at this level\")\n",
        "\n",
        "        self.root = nodes[0]\n",
        "        print(\"Bulk loading completed\")\n",
        "\n",
        "    def _search_knn(self, query: np.ndarray, k: int) -> List[Tuple[float, np.ndarray]]:\n",
        "        if not self.root:\n",
        "            return []\n",
        "\n",
        "        pq = []\n",
        "        candidates = []\n",
        "\n",
        "        def process_node(node, query_dist, current_radius): # Pass current_radius as argument\n",
        "            # Initialize radius here with current_radius\n",
        "            radius = current_radius\n",
        "\n",
        "            if node.is_leaf:\n",
        "                dist = self._compute_distance(query, node.data)\n",
        "                candidates.append((dist, node.data))\n",
        "                if len(pq) < k:\n",
        "                    heapq.heappush(pq, (-dist, node.data))\n",
        "                    radius = -pq[0][0] if len(pq) == k else float('inf') # Update radius\n",
        "                elif dist < radius:\n",
        "                    heapq.heapreplace(pq, (-dist, node.data))\n",
        "                    radius = -pq[0][0] # Update radius\n",
        "                return radius # Return updated radius\n",
        "            else:\n",
        "                entries = []\n",
        "                for child, child_dist in node.routing_entries:\n",
        "                    min_dist = max(0, query_dist - child.radius)\n",
        "                    entries.append((min_dist, child, child_dist))\n",
        "\n",
        "                entries.sort(key=lambda x: x[0])\n",
        "\n",
        "                for min_dist, child, child_dist in entries:\n",
        "                    if min_dist <= radius:\n",
        "                        child_query_dist = self._compute_distance(query, child.data)\n",
        "                        if child_query_dist - child.radius <= radius:\n",
        "                            radius = process_node(child, child_query_dist, radius) # Pass and return radius\n",
        "                return radius # Return updated radius\n",
        "\n",
        "        initial_radius = float('inf')\n",
        "        root_dist = self._compute_distance(query, self.root.data)\n",
        "        final_radius = process_node(self.root, root_dist, initial_radius) # Pass initial_radius\n",
        "\n",
        "        # Sort candidates by distance and return k nearest\n",
        "        candidates.sort(key=lambda x: x[0])\n",
        "        return candidates[:k]\n",
        "\n",
        "    def knn_search(self, queries: np.ndarray, k: int) -> List[List[Tuple[float, np.ndarray]]]:\n",
        "        results = []\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            futures = [executor.submit(self._search_knn, query, k) for query in queries]\n",
        "            results = [future.result() for future in futures]\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfcimGBpL1QX",
        "outputId": "408520b2-8946-4227-bffe-e13d8f01316e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting bulk load...\n",
            "Created 5 nodes at this level\n",
            "Created 1 nodes at this level\n",
            "Bulk loading completed\n"
          ]
        }
      ],
      "source": [
        "mtree = MTree(max_node_size=20)\n",
        "mtree.bulk_load(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1CpTEOY-MLIZ"
      },
      "outputs": [],
      "source": [
        "results = mtree.knn_search(query_data[:15], k=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MtcX6oSxSDmp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def create_vector_to_index_mapping(train_data, decimals=10):\n",
        "    \"\"\"\n",
        "    Create a mapping from vector tuple to index for fast lookup.\n",
        "\n",
        "    Args:\n",
        "        train_data: numpy array of shape (1000000, 960)\n",
        "        decimals: number of decimal places to round to\n",
        "    \"\"\"\n",
        "    # Round the data to handle floating point precision\n",
        "    rounded_data = np.round(train_data, decimals=decimals)\n",
        "\n",
        "    # Create mapping of vector to index\n",
        "    vector_to_idx = {}\n",
        "    for idx, vector in enumerate(rounded_data):\n",
        "        vector_to_idx[tuple(vector)] = idx\n",
        "\n",
        "    return vector_to_idx\n",
        "\n",
        "def batch_find_indices(vectors, vector_to_idx_map, decimals=10):\n",
        "    \"\"\"\n",
        "    Find indices for multiple vectors at once using the precomputed mapping.\n",
        "\n",
        "    Args:\n",
        "        vectors: numpy array of vectors\n",
        "        vector_to_idx_map: dict mapping vector tuples to indices\n",
        "        decimals: number of decimal places to round to\n",
        "    \"\"\"\n",
        "    rounded_vectors = np.round(vectors, decimals=decimals)\n",
        "    indices = []\n",
        "    for v in rounded_vectors:\n",
        "        indices.append(vector_to_idx_map.get(tuple(v), -1))\n",
        "    return np.array(indices)\n",
        "\n",
        "def evaluate_knn_accuracy_optimized(knn_results, ground_truth_indices, train_data):\n",
        "    \"\"\"\n",
        "    Optimized version of KNN accuracy evaluation.\n",
        "\n",
        "    Args:\n",
        "        knn_results: numpy array of shape (num_queries, num_neighbors) containing neighbor vectors\n",
        "        ground_truth_indices: numpy array of shape (num_queries, num_neighbors) containing true indices\n",
        "        train_data: numpy array of shape (num_train, vector_dim) containing training data\n",
        "    \"\"\"\n",
        "    # Create vector to index mapping once\n",
        "    vector_to_idx_map = create_vector_to_index_mapping(train_data)\n",
        "\n",
        "    num_queries = len(knn_results)\n",
        "    num_neighbors = len(knn_results[0])\n",
        "\n",
        "    # Process each query point\n",
        "    predicted_indices = []\n",
        "    for query_neighbors in knn_results:\n",
        "        # Find indices for all neighbors of this query point\n",
        "        query_indices = batch_find_indices(query_neighbors, vector_to_idx_map)\n",
        "        predicted_indices.append(query_indices)\n",
        "\n",
        "    predicted_indices = np.array(predicted_indices)\n",
        "\n",
        "    # Compute accuracy using vectorized operations\n",
        "    correct_predictions = np.sum([\n",
        "        np.isin(ground_truth_indices[i], predicted_indices[i]).sum()\n",
        "        for i in range(num_queries)\n",
        "    ])\n",
        "\n",
        "    accuracy = correct_predictions / (num_queries * num_neighbors)\n",
        "    return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wnkrJv2FXdJm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy :  97.52\n"
          ]
        }
      ],
      "source": [
        "accuracy = evaluate_knn_accuracy_optimized(results, ground_truth[:15], train_data)\n",
        "print(\"Accuracy : \",accuracy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
